[project]
name = "llm-server"
version = "0.1.0"
description = "Gemma 3 27B On-Premise LLM API Server"
readme = "README.md"
requires-python = ">=3.10"
dependencies = [
    "fastapi>=0.109.0",
    "uvicorn>=0.27.0",
    "python-multipart>=0.0.6",
    "transformers>=4.42.3",
    "accelerate>=0.30.0",
    "torch>=2.6.0",
    "pillow>=10.0.0",
]

[build-system]
requires = ["hatchling"]
build-backend = "hatchling.build"

[tool.hatch.build.targets.wheel]
packages = ["src"]

[[tool.uv.index]]
name = "pytorch"
url = "https://download.pytorch.org/whl/cu124"
explicit = true

[tool.uv.sources.torch]
index = "pytorch"
marker = "platform_machine == 'x86_64' and sys_platform != 'darwin'"
